{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mndata = MNIST('samples')\n",
    "train_X, train_y = mndata.load_training()\n",
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "test_X, test_y = mndata.load_testing()\n",
    "test_X = np.array(test_X)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.reshape(train_X[0,:],(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand=np.arange(60000)\n",
    "np.random.shuffle(rand)\n",
    "train_no=rand[:50000]\n",
    "\n",
    "val_no=np.setdiff1d(rand,train_no)\n",
    "\n",
    "X_train,X_val = train_X[train_no,:], train_X[val_no,:]\n",
    "Y_train,Y_val = train_y[train_no], train_y[val_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(x,y):\n",
    "    \n",
    "    layer=np.random.uniform(-1.,1.,size=(x,y))/np.sqrt(x*y)\n",
    "    return layer.astype(np.float32)\n",
    "\n",
    "np.random.seed(42)\n",
    "l1=init(784,128)\n",
    "l2=init(128,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid funstion\n",
    "def sigmoid(x):\n",
    "    return 1/(np.exp(-x)+1)    \n",
    "\n",
    "#derivative of sigmoid\n",
    "def d_sigmoid(x):\n",
    "    return (np.exp(-x))/((np.exp(-x)+1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax\n",
    "def softmax(x):\n",
    "    exp_element=np.exp(x-x.max())\n",
    "    return exp_element/np.sum(exp_element,axis=0)\n",
    "\n",
    "#derivative of softmax\n",
    "def d_softmax(x):\n",
    "    exp_element=np.exp(x-x.max())\n",
    "    return exp_element/np.sum(exp_element,axis=0)*(1-exp_element/np.sum(exp_element,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward and backward pass\n",
    "def forward_backward_pass(x,y):\n",
    "    targets = np.zeros((len(y),10), np.float32)\n",
    "    targets[range(targets.shape[0]),y] = 1\n",
    " \n",
    "    \n",
    "    x_l1=x.dot(l1)\n",
    "    x_sigmoid=sigmoid(x_l1)\n",
    "    x_l2=x_sigmoid.dot(l2)\n",
    "    out=softmax(x_l2)\n",
    "   \n",
    " \n",
    "    error=2*(out-targets)/out.shape[0]*d_softmax(x_l2)\n",
    "    update_l2=np.matmul(x_sigmoid.T,error)\n",
    "    \n",
    "    \n",
    "    error=((l2).dot(error.T)).T*d_sigmoid(x_l1)\n",
    "    update_l1=np.matmul(x.T,error)\n",
    "\n",
    "    return out,update_l1,update_l2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0th epoch: train accuracy: 0.109 | validation accuracy:0.072\n",
      "For 10th epoch: train accuracy: 0.250 | validation accuracy:0.236\n",
      "For 20th epoch: train accuracy: 0.391 | validation accuracy:0.435\n",
      "For 30th epoch: train accuracy: 0.516 | validation accuracy:0.552\n",
      "For 40th epoch: train accuracy: 0.594 | validation accuracy:0.616\n",
      "For 50th epoch: train accuracy: 0.703 | validation accuracy:0.661\n",
      "For 60th epoch: train accuracy: 0.648 | validation accuracy:0.688\n",
      "For 70th epoch: train accuracy: 0.773 | validation accuracy:0.705\n",
      "For 80th epoch: train accuracy: 0.773 | validation accuracy:0.725\n",
      "For 90th epoch: train accuracy: 0.711 | validation accuracy:0.739\n",
      "For 100th epoch: train accuracy: 0.766 | validation accuracy:0.747\n",
      "For 110th epoch: train accuracy: 0.797 | validation accuracy:0.755\n",
      "For 120th epoch: train accuracy: 0.734 | validation accuracy:0.758\n",
      "For 130th epoch: train accuracy: 0.797 | validation accuracy:0.765\n",
      "For 140th epoch: train accuracy: 0.812 | validation accuracy:0.766\n",
      "For 150th epoch: train accuracy: 0.828 | validation accuracy:0.767\n",
      "For 160th epoch: train accuracy: 0.773 | validation accuracy:0.773\n",
      "For 170th epoch: train accuracy: 0.805 | validation accuracy:0.778\n",
      "For 180th epoch: train accuracy: 0.797 | validation accuracy:0.781\n",
      "For 190th epoch: train accuracy: 0.844 | validation accuracy:0.783\n",
      "For 200th epoch: train accuracy: 0.852 | validation accuracy:0.785\n",
      "For 210th epoch: train accuracy: 0.797 | validation accuracy:0.790\n",
      "For 220th epoch: train accuracy: 0.844 | validation accuracy:0.793\n",
      "For 230th epoch: train accuracy: 0.805 | validation accuracy:0.796\n",
      "For 240th epoch: train accuracy: 0.805 | validation accuracy:0.799\n",
      "For 250th epoch: train accuracy: 0.781 | validation accuracy:0.798\n",
      "For 260th epoch: train accuracy: 0.828 | validation accuracy:0.800\n",
      "For 270th epoch: train accuracy: 0.781 | validation accuracy:0.801\n",
      "For 280th epoch: train accuracy: 0.867 | validation accuracy:0.802\n",
      "For 290th epoch: train accuracy: 0.867 | validation accuracy:0.802\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "lr = 0.01\n",
    "batch = 128\n",
    "\n",
    "losses,accuracies,val_accuracies=[],[],[]\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample=np.random.randint(0,X_train.shape[0],size=(batch))\n",
    "    x=X_train[sample].reshape((-1,28*28))\n",
    "    y=Y_train[sample]\n",
    " \n",
    "\n",
    "    out,update_l1,update_l2=forward_backward_pass(x,y)\n",
    "  \n",
    "    category=np.argmax(out,axis=1)\n",
    "    accuracy=(category==y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    loss=((category-y)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    l1=l1-lr*update_l1\n",
    "    l2=l2-lr*update_l2\n",
    "    \n",
    "    if(i%2==0):    \n",
    "        X_val=X_val.reshape((-1,28*28))\n",
    "        val_out=np.argmax(softmax(sigmoid(X_val.dot(l1)).dot(l2)),axis=1)\n",
    "        val_acc=(val_out==Y_val).mean()\n",
    "        val_accuracies.append(val_acc.item())\n",
    "    if(i%10==0): print('For %dth epoch: train accuracy: %.3f | validation accuracy:%.3f' % (i, accuracy, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12 64-bit",
   "name": "python2712jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}